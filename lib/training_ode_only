import time
import numpy as np
import torch
import torch.optim as optim
from torch.utils import data
from torchdiffeq import odeint_adjoint as odeint
import tools
from dataset import Dataset
from pickle import load
from os import remove
from glob import glob
import torch.nn as nn

DEVICE = ("cuda" if torch.cuda.is_available() else "cpu")


"""
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICE"] = "0"
"""

PATH_DATA = '../data/'
NB_FOLDS = 1
LEARNING_RATE = 0.0001
NUM_EPOCHS = 40


class LatentODEfunc(nn.Module):

    def __init__(self, latent_dim=2, nhidden=20):
        super(LatentODEfunc, self).__init__()
        self.elu = nn.ELU(inplace=True)
        self.fc1 = nn.Linear(latent_dim, nhidden)
        self.fc2 = nn.Linear(nhidden, nhidden)
        self.fc3 = nn.Linear(nhidden, latent_dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.fc1(x)
        out = self.elu(out)
        out = self.fc2(out)
        out = self.elu(out)
        out = self.fc3(out)
        return out


if __name__ == "__main__":

    unscale = lambda x: x*(MAXI_FVC-MINI_FVC) + MINI_FVC
    
    for f in glob(f"{PATH_DATA}/histo-fold/histo-fold-*.pt"): #Removes existing histo-fold-X.pt
        remove(f)

    with open('minmax.pickle', 'rb') as minmax_file:
        dict_extremum = load(minmax_file)
        
    MINI_FVC = dict_extremum['FVC']["min"]
    MAXI_FVC = dict_extremum['FVC']["max"]
    MEAN_week = dict_extremum['Weeks']["mean"]
    STD_week = dict_extremum['Weeks']["std"]
    FOLD_LABELS = np.load("4-folds-split.npy")
    
    for k in range(NB_FOLDS):
        indices_train, indices_test = tools.train_test_indices(FOLD_LABELS, k)
        func = LatentODEfunc().to(DEVICE)
        # func.load_state_dict(torch.load(f'../data/model/model-{k}.pth')['state_dict'])
        
        optimiser = optim.Adam(func.parameters(), lr=LEARNING_RATE, weight_decay=5e-8)
    
        #####################
        # Loding of data
        #####################
    
        training_set = Dataset(PATH_DATA, indices_train)
        training_generator = data.DataLoader(training_set, batch_size=1, shuffle=True)
    
        testing_set = Dataset(PATH_DATA, indices_test)
        testing_generator = data.DataLoader(testing_set, batch_size=1, shuffle=False)
        
        histo = torch.zeros((NUM_EPOCHS, 2))
    
        for epoch in range(NUM_EPOCHS):
            start_time = time.time()
            loss_train, loss_test = 0, 0

           #TRAINING    
            for scans, misc, FVC, percent, weeks in training_generator:
                ranger = np.where(weeks != 0)[1]
                misc = misc[:,ranger[0],:].squeeze(1) #Dépend du m
                fvc = FVC[:,ranger[0]]
                std = torch.ones_like(fvc)*.7
                fvc = torch.cat((fvc, std),1)
                percent = percent[:,ranger[0]]
                weeks = weeks[:,ranger]
                weeks = weeks - weeks[:,0]
                scans, misc = scans.to(DEVICE), misc.to(DEVICE)
                fvc, percent, weeks = fvc.to(DEVICE), percent.to(DEVICE), weeks.to(DEVICE)
                
                # Clear stored gradient
                optimiser.zero_grad()
                pred = odeint(func, fvc, weeks.squeeze()).permute(1,0,2)
                
                #Deprocessing
                mean = unscale(pred[:, :, 0])
                std = pred[:, :, 1]*100
                goal = FVC[:,ranger]
                goal = unscale(goal).to(DEVICE)
                loss = tools.ode_laplace_log_likelihood(goal, mean, std, epoch, 30)
                loss_train += loss
                loss.backward() # Gradient Computation
                optimiser.step() # Update parameters

            #VALIDATION
            with torch.no_grad():
                func.eval()
                for scans, misc, FVC, percent, weeks in testing_generator:
                    ranger = np.where(weeks != 0)[1]
                    misc = misc[:,ranger[0],:].squeeze(1) #Dépend du m
                    fvc = FVC[:,ranger[0]]
                    std = torch.ones_like(fvc)*.7
                    fvc = torch.cat((fvc, std),1)
                    percent = percent[:,ranger[0]]
                    weeks = weeks[:,ranger]
                    weeks = weeks - weeks[:,0]
                    scans, misc = scans.to(DEVICE), misc.to(DEVICE)
                    fvc, percent, weeks = fvc.to(DEVICE), percent.to(DEVICE), weeks.to(DEVICE)
                    pred = odeint(func, fvc, weeks.squeeze()).permute(1,0,2)
                    
                    #Deprocessing
                    mean = unscale(pred[:, :, 0])
                    std = pred[:, :, 1]*100 
                    goal = FVC[:,ranger]
                    goal = unscale(goal).to(DEVICE)
                    loss = tools.ode_laplace_log_likelihood(goal, mean, std, epoch, 30)
                    loss_test += loss
                    
            loss_train = loss_train/len(training_generator)
            loss_test = loss_test/len(testing_generator)
            print(f'| Epoch: {epoch+1} | Train Loss: {loss_train:.3f} | Test. Loss: {loss_test:.3f} |')
            histo[epoch, 0] = loss_train
            histo[epoch, 1] = loss_test
            
        DATA_SAVE = {'weeks': weeks, 'fvc': fvc, 'misc': misc, 'goal': goal, 'mean': mean, 'std': std}
        torch.save(DATA_SAVE, f"{PATH_DATA}/saved_data/data_save.pt")        
        torch.save(histo, f"{PATH_DATA}/histo-fold/histo-fold-{k}.pt")
        
        CHECKPOINT = {'model': func,
                  'state_dict': func.state_dict(),
                  'optimiser' : optimiser.state_dict()}
        torch.save(CHECKPOINT, f'../data/model/model-{k}.pth')
